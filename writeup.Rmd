---
title: "Practical Machine Learning Project"
author: "Peter Saunders"
date: "Sunday, January 18, 2015"
output: html_document
---

Project Aims
------------------------
The aim of this project is to produce a machine learning algorithm to predict the quality with which a bicep curl 
was performed from data obtained from sensors on the dumbbell and the athlete's belt, arm and forearm.


Data Exploration and Cleaning
------------------------

I began by loading the Weight Lifting Exercise training dataset into R to explore its structure.  I used the functions `str` and
`summary` to look at the structure of the data.

```{r message=FALSE}
#Load packages
require(caret)
require(randomForest)

#Load training data
trainingDf = read.csv("./data/pml-training.csv")
```

The raw data-frame contains 19622 rows and 160 columns but, looking at the summary information, I was able to identify 100 columns which contained mostly missing (either NA or empty string) data.

```{r}
#Find columns with NAs or empty strings
naCount      = apply(trainingDf, 2, function(x)length(which(is.na(x))))
missingCount = apply(trainingDf, 2, function(x)length(which(x == "")))
summaryColumns = sort(names(which(naCount > 0 | missingCount > 0)))
```

These column names all had the prefixes:

- amplitude
- avg
- kurtosis
- max
- min
- skewness
- stddev
- var

These columns only contain numeric values in the rows where `new_window = yes` and the names suggest that these are summary
statistics for each window.  Since we are aiming to predict the class from a single row I decided to discard these columns and those
referring to the windows, `new_window` and `num_window`.

```{r}
#Select the non-summary columns
windowColumns  = which(names(trainingDf) %in% c("new_window", "num_window"))
summaryColumns = which(naCount > 0 | missingCount > 0)
trainingDf = trainingDf[, -c(windowColumns, summaryColumns)]
```

The data-frame now contains 58 columns and has no missing values.  The columns are:

- 1 index column, `X`
- 1 user column, `user_name`
- 3 timestamp columns
- 52 data columns from the sensors on the belt, arm, dumbbell and forearm
- 1 outcome column, `classe`


I decided to also remove the index, user and timestamp columns.  Because the data is ordered by the 'classe' (A-E)
variable it will be highly correlated with X and the timestamp variables, although these are not meaningful as
predictors outside of this particular training set and would give a highly biased estimate for the out-of-sample error.

```{r}
#Remove index, user and timestamp columns
trainingDf = trainingDf[, 6:58]
```


Model Building and Cross-Validation
------------------------
I decided to use k-fold cross-validation and a random-forest method to build a prediction algorithm.

I started by splitting the data into the training/testing set and a validation set.  I used ~80% of the 
data for training/testing and the remaining 20% for validation.


```{r}
#Separate data into training and validation sets
#and remove index, user and timestamp columns
set.seed(3485)
modelIdx = createDataPartition(trainingDf$classe, p = 0.8, list = FALSE)

modelData       = trainingDf[modelIdx, ]
validationData  = trainingDf[-modelIdx, ]
```

I then ran K-fold cross-validation, each time building a random forest and storing that model.

I tried several different values and found that K = 15 was a good compromise between bias and variance and did not take
too long to evaluate.

```{r}
#Now do k-fold cross-validation
K = 15
kfolds = createFolds(modelData$classe, k = K, list = TRUE, returnTrain = TRUE)
models = vector(length = K, mode = "list")

for (i in 1:K) {
    kthTraining = modelData[kfolds[[i]], ]  #make training
    kthTesting  = modelData[-kfolds[[i]], ] #make testing
    
    #Make model and store it
    kthRF = randomForest(classe ~., data=kthTraining, ntree=500, norm.votes=FALSE)
    models[[i]] = kthRF
    
    #Evaluate on testing set to print accuracy
    kthPreds = predict(kthRF, newdata = kthTesting)
    
    cm = confusionMatrix(kthPreds, kthTesting$classe)
    cat("Accuracy[", i, "] = ", cm$overall['Accuracy'], "\n", sep="")
}
```

To estimate the out-of-sample error I ran each of the models on the validation data-set.

```{r}
#Use all of the models to make predictions
validationPreds = lapply(models, function(x){predict(x, newdata = validationData)})
validationMatrix = matrix(unlist(validationPreds), ncol = nrow(validationData), nrow = K, byrow = TRUE)
```

I then used a 'vote' of the model results to select the most common (modal) prediction from the K models.

```{r}
#Use vote of each of the K models to form prediction
vote <- function(x) {
    ux = unique(x)
    ux[which.max(tabulate(match(x, ux)))]
}

finalPredict = apply(validationMatrix, MARGIN = 2, vote)
```

I then looked at the measures of accuracy and the confusion matrix to estimate the out-of-sample error.

```{r}
#Examine confusion matrix and statistics
finalCm = confusionMatrix(finalPredict, validationData$classe)

cat("Final validation accuracy =", finalCm$overall['Accuracy'], "\n")
print(finalCm$table)
```

I decided to look at the most important variables and plot them to see how separable the classes
are within these variables.

```{r message=FALSE, fig.width=8, fig.height=8}
### Look at the most important variables in the models ###
require(AppliedPredictiveModeling)
require(ellipse)

topNImportant <- function(model, N=4) {
    imps     = importance(model)
    rankedVars = rownames(imps)[order(imps, decreasing=TRUE)]    
    topN = rankedVars[1:N]
    
    return(topN)
}

importantVars = sapply(models, topNImportant)
top4Overall   = apply(importantVars, 1, vote)
cat("Most important variables: ", paste(top4Overall, collapse=", "), "...", sep="")

featurePlot(x = validationData[, top4Overall],
            y = validationData$classe,
            plot = "pairs",
            #visual args
            auto.key = list(columns = 5))
```


Conclusions
------------------------
Using a random forest approach I was able to produce a highly accurate 
algorithm for predicting the 'classe' of the exercise performed.

Based on the results of prediction on the independent validation data-set, I estimate an **out-of-sample error rate of 0.4%**.

Plotting the most important variables shows that there is very little differentiation between the classes in any single variable.  The model is accurate because it uses a large number (51) of predictors and the aggregated random forest approach is well-suited to problems such as these.





