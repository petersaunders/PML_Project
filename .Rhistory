colours = cut2(concrete$CoarseAggregate, g = 10)
plot(concrete$CompressiveStrength, col = colours, pch=19)
colours = cut2(concrete$FineAggregate, g = 10)
plot(concrete$CompressiveStrength, col = colours, pch=19)
colours = cut2(concrete$Age, g = 10)
plot(concrete$CompressiveStrength, col = colours, pch=19)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
colours = cut2(concrete$Age, g = 10)
plot(training$CompressiveStrength, col = colours, pch=19)
colours = cut2(concrete$FlyAsh, g = 10)
plot(training$CompressiveStrength, col = colours, pch=19)
colours = cut2(concrete$Cement, g = 10)
plot(training$CompressiveStrength, col = colours, pch=19)
colours = cut2(concrete$BlastFurnaceSlag, g = 10)
plot(training$CompressiveStrength, col = colours, pch=19)
colours = cut2(concrete$BlastFurnaceSlag, g = 4)
plot(training$CompressiveStrength, col = colours, pch=19)
colours = cut2(concrete$FlyAsh)
plot(training$CompressiveStrength, col = colours, pch=19)
colours = cut2(concrete$FlyAsh, g=4)
plot(training$CompressiveStrength, col = colours, pch=19)
colours = cut2(concrete$Superplasticizer)
plot(training$CompressiveStrength, col = colours, pch=19)
#### Q3
colours = cut2(concrete$Superplasticizer, g=4)
plot(training$CompressiveStrength, col = colours, pch=19)
colours = cut2(concrete$CoarseAggregate, g=4)
plot(training$CompressiveStrength, col = colours, pch=19)
colours = cut2(concrete$FineAggregate, g=4)
plot(training$CompressiveStrength, col = colours, pch=19)
colours = cut2(concrete$Age, g=4)
plot(training$CompressiveStrength, col = colours, pch=19)
length(unique(training$FlyAsh))
hist(training$Superplasticizer)
lSp = log(training$Superplasticizer)
head(lSp)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
head(names(training))
length(names(training))
ilNames = which(grepl("^IL.*", names(training)))
names(training)[ilNames]
pcs = preProcess(data = newTraining, method="pca", thresh = 0.9)
newTraining = training[, ilNames]
pcs = preProcess(data = newTraining, method="pca", thresh = 0.9)
pcs = preProcess(newTraining, method="pca", thresh = 0.9)
summary(pcs)
pcs$pcaComp
pcs
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
ilNames = which(grepl("^IL.*", names(training)))
diagnos = which(names(training) == "diagnosis")
newTraining = training[, c(ilNames, diagnosis)]
View(newTraining)
ilNames = which(grepl("^IL.*", names(training)))
diagnos = which(names(training) == "diagnosis")
newTraining = training[, c(ilNames, diagnos)]
View(newTraining)
ncol(newTraining)
?preProcess
?train
# Model with Raw data
modRaw = train(newTraining[,-13], newTraining[,13], method = "glm")
# Model with PCA data
trCont = trainControl(preProcOptions = list(thresh = 0.8))
modPca = train(newTraining[,-13], newTraining[,13], method = "glm", preProcess = "pca", trControl = trCont)
install.packages('e1071')
# Model with Raw data
modRaw = train(newTraining[,-13], newTraining[,13], method = "glm")
# Model with PCA data
trCont = trainControl(preProcOptions = list(thresh = 0.8))
modPca = train(newTraining[,-13], newTraining[,13], method = "glm", preProcess = "pca", trControl = trCont)
confusionMatrix(testing$diagnosis, predict(modRaw, testing))
confusionMatrix(testing$diagnosis, predict(modPca, testing))
newTraining = training[, c(ilNames, diagnos)]
newTesting  = testing[, c(ilNames, diagnos)]
# Model with Raw data
modRaw = train(newTraining[,-13], newTraining[,13], method = "glm")
# Model with PCA data
trCont = trainControl(preProcOptions = list(thresh = 0.8))
modPca = train(newTraining[,-13], newTraining[,13], method = "glm", preProcess = "pca", trControl = trCont)
#Look at results
confusionMatrix(testing$diagnosis, predict(modRaw, testing))
confusionMatrix(testing$diagnosis, predict(modPca, testing))
# Model with PCA data
ppo = list(thresh = 0.8))
modPca = train(newTraining[,-13], newTraining[,13], method = "glm", preProcess = "pca", preProcOptions = ppo)
#Look at results
confusionMatrix(testing$diagnosis, predict(modRaw, testing))
confusionMatrix(testing$diagnosis, predict(modPca, testing))
newTraining = training[, c(ilNames, diagnos)]
newTesting  = testing[, c(ilNames, diagnos)]
# Model with Raw data
modRaw = train(newTraining[,-13], newTraining[,13], method = "glm")
# Model with PCA data
ppo = list(thresh = 0.8))
modPca = train(newTraining[,-13], newTraining[,13], method = "glm", preProcess = "pca", preProcOptions = ppo)
#Look at results
confusionMatrix(newTesting$diagnosis, predict(modRaw, newTesting))
confusionMatrix(newTesting$diagnosis, predict(modPca, newTesting))
# Model with PCA data
ppo = list(thresh = 0.8)
modPca = train(newTraining[,-13], newTraining[,13], method = "glm", preProcess = "pca", preProcOptions = ppo)
#Look at results
confusionMatrix(newTesting$diagnosis, predict(modRaw, newTesting))
confusionMatrix(newTesting$diagnosis, predict(modPca, newTesting))
# Model with PCA data
ppo = list(thresh = 0.8, ICAcomp = 3, k = 5)
trc = trainControl(preProcOptions = ppo)
modPca = train(newTraining[,-13], newTraining[,13], method = "glm", preProcess = "pca", trControl = trCont)
confusionMatrix(newTesting$diagnosis, predict(modPca, newTesting))
View(newTesting)
?trainControl
# Model with PCA data
ppo = list(thresh = 0.8, ICAcomp = 3, k = 5)
trc = trainControl(method="none", preProcOptions = ppo)
modPca = train(newTraining[,-13], newTraining[,13], method = "glm", preProcess = "pca", trControl = trCont)
confusionMatrix(newTesting$diagnosis, predict(modPca, newTesting))
#### Q3
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
hist(training$Superplasticizer)
length(unique(training$Superplasticizer))
nrow(training)
summary(training$Superplasticizer)
hist(log(1+training$Superplasticizer))
install.packages('AppliedPredictiveModeling')
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
set.seed(125)
modelFit =  train(Case ~ ., data = training, method = 'rpart')
inTrain = createDataPartition(y = segmentationOriginal$Case, p=0.7, list=FALSE)
training = segmentationOriginal[inTrain,]
testing  = segmentationOriginal[-inTrain,]
set.seed(125)
modelFit =  train(Case ~ ., data = training, method = 'rpart')
modelFit$finalModel
head(segmentationOriginal)
set.seed(125)
modelFit =  train(Class ~ ., data = training, method = 'rpart')
modelFit$finalModel
plot(modelFit$finalModel)
install.packages('rattle')
require(rattle)
fancyRpartPlot(modelFit$finalModel)
modelFit$finalModel
set.seed(125)
modelFit =  train(Class ~ ., data = training, method = 'rpart')
modelFit$finalModel
head(segmentationOriginal)
unique(segmentationOriginal$Case)
inTrain = which(segmentationOriginal$Case == "Train")
inTrain = which(segmentationOriginal$Case == "Train")
length(inTrain)
inTrain = which(segmentationOriginal$Case == "Train")
training = segmentationOriginal[inTrain,]
testing  = segmentationOriginal[-inTrain,]
set.seed(125)
modelFit =  train(Class ~ ., data = training, method = 'rpart')
modelFit$finalModel
library(pgmm)
data(olive)
olive = olive[,-1]
install.packages(pgmm)
install.packages('pgmm'')
install.packages('pgmm')
library(pgmm)
data(olive)
olive = olive[,-1]
head(olive)
oliveModel = train(Area ~., data=olive, method='rpart')
oliveModel$finalModel
install.packages('tree')
require(tree)
head(olive)
oliveTree = tree(Area ~ ., data=olive)
oliveTree
?tree
predict(oliveTree, newdata)
newdata = as.data.frame(t(colMeans(olive)))
predict(oliveTree, newdata)
table(olive$Area)
head(train)
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
install.packages(ElemStatLearn)
install.packages('ElemStatLearn'')
install.packages('ElemStatLearn')
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
head(trainSA)
set.seed(13234)
hdModel = train(chd ~ age+alcohol+obesity+tobacco+typea+ldl, method="glm", family="binomial")
hdModel = train(chd ~ age+alcohol+obesity+tobacco+typea+ldl, data=trainSA, method="glm", family="binomial")
hdModel
set.seed(13234)
hdModel = train(chd ~ age+alcohol+obesity+tobacco+typea+ldl, data=trainSA, method="glm", family="binomial")
trainPred = predict(hdModel, newdata=trainSA)
testPred  = predict(hdModel, newdata=testSA)
str(testPred)
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
trainMC = missClass(trainSA$chd, trainPred)
testMC  = missClass(testSA$chd, testPred)
trainMC
testMC
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
head(vowel.test)
head(vowel.train)
vowel.train$y = as.factor(vowel.train$y)
vowel.test$y = as.factor(vowel.test$y)
install.packages(randomForest)
install.packages('randomForest')
require(randomForest)
set.seed(33833)
vowelForest = randomForest(y ~., data=vowel.train)
str(vowelForest)
varImp(vowelForest, useModel = 0)
importance(vowelForest)
sort(importance(vowelForest))
important(vowelForest)[order(importance(vowelForest))]
importance(vowelForest)[order(importance(vowelForest))]
imps = importance(vowelForest)
imps
rownames(imps)
rownames(imps)[order(imps, decreasing = TRUE)]
require(caret)
setwd("D:/Documents/Other Courses/Practical Machine Learning/Project")
#Model building
require(caret)
require(randomForest)
#Initial Exploration of Project data
#setwd("D:/Documents/Other Courses/Practical Machine Learning/Project")
#Load data
trainDf = read.csv("./data/pml-training.csv")
testDf  = read.csv("./data/pml-testing.csv")
#Look at structure
#str(trainDf)
#summary(trainDf)
#Some columns contain a large number of NAs or empty fields
naCount      = apply(trainDf, 2, function(x)length(which(is.na(x))))
missingCount = apply(trainDf, 2, function(x)length(which(x == "")))
summaryColumns = sort(names(which(naCount > 0 | missingCount > 0)))
#Select the non-summary columns
windowColumns  = which(names(trainDf) %in% c("new_window", "num_window"))
summaryColumns = which(naCount > 0 | missingCount > 0)
trainSubDf = trainDf[, -c(windowColumns, summaryColumns)]
#Separate data into training and validation sets
#and remove index, user and timestamp columns
set.seed(3485)
modelIdx = createDataPartition(trainSubDf$classe, p = 0.8, list = FALSE)
modelData       = trainSubDf[modelIdx, 6:58]
validationData  = trainSubDf[-modelIdx, 6:58]
#Now do k-fold cross-validation
require(randomForest)
K = 15
kfolds = createFolds(modelData$classe, k = K, list = TRUE, returnTrain = TRUE)
models = vector(length = K, mode = "list")
for (i in 1:K) {
training = modelData[kfolds[[i]], ]  #make training
testing  = modelData[-kfolds[[i]], ] #make testing
#Make model and store it
rfMod = randomForest(classe ~., data=training, ntree=500, norm.votes=FALSE)
models[[i]] = rfMod
#Evaluate on testing set to estimate accuracy
preds = predict(rfMod, newdata=testing)
accuracy = length(which(preds == testing$classe)) / nrow(testing)
cat("Estimated Test Accuracy = ", accuracy, "\n", sep="")
}
str(models, max.level=1)
validationPreds = lapply(models, function(x){predict(x, newdata=validationData)})
validationPreds[[1]]
validationPreds = lapply(models, function(x){predict(x, newdata=validationData)})
length(validationPreds[[1]])
View(validationPreds[[1]])
p1 = as.character(valdationPreds[[1]])
p1 = as.character(validationPreds[[1]])
length(p1)
head(p1)
validationPreds = lapply(models, function(x){as.character(predict(x, newdata=validationData))})
str(validationPreds)
validationMatrix = unlist(validationPreds)
str(validationMatrix)
validationMatrix = matrix(unlist(validationPreds), ncol=nrow(validationData), nrow=K)
dim(validationMatrix)
validationMatrix[, 1]
validationMatrix[, 1:10]
validationMatrix[, 200:210]
validationMatrix[, 190:210]
#Use consensus vote of each of the K models to form prediction
vote <- function(x) {
ux = unique(x)
ux[which.max(tabulate(match(x, ux)))]
}
finalPredict = apply(validationMatrix, MARGIN = 2, vote)
finalPredict[190:210]
validationAccuracy = length(which(finalPredict != as.character(validationData$classe))) / nrow(validationData)
validationAccuracy
brokeBits = which(finalPredict != as.character(validationData$classe))
head(brokeBits)
validationMatrix[, head(brokeBits, 10)]
as.character(validationData$classe)[, head(brokeBits, 10)]
as.character(validationData$classe)[head(brokeBits, 10)]
validationMatrix[, 75:95]
as.character(validationData$classe)[75:95]
as.character(validationData$classe)[70:95]
validationMatrix[, 70:85]
#Now use all of the models to make predictions
#TODO: Something is broken here... this definitely is not right!!!
validationPreds = lapply(models, function(x){as.character(predict(x, newdata=validationData))})
validationMatrix = matrix(unlist(validationPreds), ncol=nrow(validationData), nrow=K)
#Use consensus vote of each of the K models to form prediction
vote <- function(x) {
ux = unique(x)
ux[which.max(tabulate(match(x, ux)))]
}
finalPredict = apply(validationMatrix, MARGIN = 2, vote)
validationAccuracy = length(which(finalPredict != as.character(validationData$classe))) / nrow(validationData)
cat("Final validation accuracy =", validationAccuracy, "\n")
K = 20
kfolds = createFolds(modelData$classe, k = K, list = TRUE, returnTrain = TRUE)
models = vector(length = K, mode = "list")
for (i in 1:K) {
training = modelData[kfolds[[i]], ]  #make training
testing  = modelData[-kfolds[[i]], ] #make testing
#Make model and store it
rfMod = randomForest(classe ~., data=training, ntree=500, norm.votes=FALSE)
models[[i]] = rfMod
#Evaluate on testing set to estimate accuracy
preds = predict(rfMod, newdata=testing)
accuracy = length(which(preds == testing$classe)) / nrow(testing)
cat("Estimated Test Accuracy = ", accuracy, "\n", sep="")
}
K = 10
kfolds = createFolds(modelData$classe, k = K, list = TRUE, returnTrain = TRUE)
models = vector(length = K, mode = "list")
for (i in 1:K) {
training = modelData[kfolds[[i]], ]  #make training
testing  = modelData[-kfolds[[i]], ] #make testing
#Make model and store it
rfMod = randomForest(classe ~., data=training, ntree=500, norm.votes=FALSE)
models[[i]] = rfMod
#Evaluate on testing set to estimate accuracy
preds = predict(rfMod, newdata=testing)
accuracy = length(which(preds == testing$classe)) / nrow(testing)
cat("Estimated Test Accuracy = ", accuracy, "\n", sep="")
}
#Now use all of the models to make predictions
validationPreds = lapply(models, function(x){as.character(predict(x, newdata=validationData))})
validationMatrix = matrix(unlist(validationPreds), ncol=nrow(validationData), nrow=K)
#Use consensus vote of each of the K models to form prediction
vote <- function(x) {
ux = unique(x)
ux[which.max(tabulate(match(x, ux)))]
}
finalPredict = apply(validationMatrix, MARGIN = 2, vote)
validationAccuracy = length(which(finalPredict != as.character(validationData$classe))) / nrow(validationData)
cat("Final validation accuracy =", validationAccuracy, "\n")
#and remove index, user and timestamp columns
set.seed(3485)
modelIdx = createDataPartition(trainSubDf$classe, p = 0.8, list = FALSE)
modelData       = trainSubDf[modelIdx, 6:58]
validationData  = trainSubDf[-modelIdx, 6:58]
#Now do k-fold cross-validation
require(randomForest)
K = 10
kfolds = createFolds(modelData$classe, k = K, list = TRUE, returnTrain = TRUE)
models = vector(length = K, mode = "list")
for (i in 1:K) {
training = modelData[kfolds[[i]], ]  #make training
testing  = modelData[-kfolds[[i]], ] #make testing
#Make model and store it
rfMod = randomForest(classe ~., data=training, ntree=500, norm.votes=FALSE)
models[[i]] = rfMod
#Evaluate on testing set to estimate accuracy
preds = predict(rfMod, newdata=testing)
accuracy = length(which(preds == testing$classe)) / nrow(testing)
cat("Estimated Test Accuracy = ", accuracy, "\n", sep="")
}
#Now use all of the models to make predictions
validationPreds = lapply(models, function(x){as.character(predict(x, newdata=validationData))})
validationMatrix = matrix(unlist(validationPreds), ncol=nrow(validationData), nrow=K)
#Use consensus vote of each of the K models to form prediction
vote <- function(x) {
ux = unique(x)
ux[which.max(tabulate(match(x, ux)))]
}
finalPredict = apply(validationMatrix, MARGIN = 2, vote)
validationAccuracy = length(which(finalPredict != as.character(validationData$classe))) / nrow(validationData)
cat("Final validation accuracy =", validationAccuracy, "\n")
#Now use all of the models to make predictions
validationPreds = lapply(models, function(x){predict(x, newdata=validationData)})
validationMatrix = matrix(unlist(validationPreds), ncol=nrow(validationData), nrow=K)
#Use consensus vote of each of the K models to form prediction
vote <- function(x) {
ux = unique(x)
ux[which.max(tabulate(match(x, ux)))]
}
finalPredict = apply(validationMatrix, MARGIN = 2, vote)
validationAccuracy = length(which(finalPredict != validationData$classe)) / nrow(validationData)
cat("Final validation accuracy =", validationAccuracy, "\n")
?confusionMatrix
confusionMatrix(finalPredict, validationData$classe)
cat("Final validation accuracy =", validationAccuracy, "\n")
validationAccuracy = length(which(finalPredict != validationData$classe)) / nrow(validationData)
cat("Final validation accuracy =", validationAccuracy, "\n")
validationAccuracy = length(which(finalPredict == validationData$classe)) / nrow(validationData)
validationAccuracy
validationAccuracy = length(which(validationPreds[[1]] == validationData$classe)) / nrow(validationData)
validationAccuracy
validationAccuracy = length(which(validationPreds[[2]] == validationData$classe)) / nrow(validationData)
validationAccuracy
#Now use all of the models to make predictions
validationPreds = lapply(models, function(x){predict(x, newdata=validationData)})
validationMatrix = matrix(unlist(validationPreds), ncol=nrow(validationData), nrow=K, byrow = TRUE)
#Use consensus vote of each of the K models to form prediction
vote <- function(x) {
ux = unique(x)
ux[which.max(tabulate(match(x, ux)))]
}
finalPredict = apply(validationMatrix, MARGIN = 2, vote)
validationAccuracy = length(which(validationPreds[[1]] == validationData$classe)) / nrow(validationData)
validationAccuracy = length(which(finalPredict == validationData$classe)) / nrow(validationData)
cat("Final validation accuracy =", validationAccuracy, "\n")
#Plot a table
confusionMatrix(finalPredict, validationData$classe)
cm = confusionMatrix(finalPredict, validationData$classe)
str(cm)
cm$overall
cm$overall['Accuracy']
#Use all of the models to make predictions
validationPreds = lapply(models, function(x){predict(x, newdata=validationData)})
validationMatrix = matrix(unlist(validationPreds), ncol=nrow(validationData), nrow=K, byrow = TRUE)
#Use consensus vote of each of the K models to form prediction
vote <- function(x) {
ux = unique(x)
ux[which.max(tabulate(match(x, ux)))]
}
finalPredict = apply(validationMatrix, MARGIN = 2, vote)
#Examine confusion matrix and statistics
cm = confusionMatrix(finalPredict, validationData$classe)
cat("Final validation accuracy =", cm$overall['Accuracy'], "\n")
image(cm$table)
cm$table
cat(cm$table)
printcat(cm$table)
print(cm$table)
cat("Final validation accuracy =", cm$overall['Accuracy'], "\n")
print(cm$table)
cm = confusionMatrix(preds, testing$classe)
cat("Accuracy[", i, "] = ", cm$overall['Accuracy'], "\n", sep="")
K
#Examine confusion matrix and statistics
finalCm = confusionMatrix(finalPredict, validationData$classe)
cat("Final validation accuracy =", finalCm$overall['Accuracy'], "\n")
print(finalCm$table)
testSubDf = testDf[, 6:58]
testSetPreds = lapply(models, function(x){predict(x, newdata=testSubDf)})
testSetMatrix = matrix(unlist(testSetPreds), ncol=nrow(testSubDf), nrow=K, byrow = TRUE)
finalPredict = apply(testSetMatrix, MARGIN = 2, vote)
nrow(testSubDf)
ncol(testSubDf)
names(testSubDf)
testSubDf = testDf[, -c(windowColumns, summaryColumns)]
testSubDf = testSubDf[, 6:58]
testSetPreds = lapply(models, function(x){predict(x, newdata=testSubDf)})
testSetMatrix = matrix(unlist(testSetPreds), ncol=nrow(testSubDf), nrow=K, byrow = TRUE)
finalPredict = apply(testSetMatrix, MARGIN = 2, vote)
finalTestPredict = apply(testSetMatrix, MARGIN = 2, vote)
finalTestPredict
answers = finalTestPredict
class(finalTestPredict)
pml_write_files = function(x){
n = length(x)
for(i in 1:n){
filename = paste0("problem_id_",i,".txt")
write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
}
}
setwd("./answers")
pml_write_files(answers)
rm(list=ls())
source('D:/Documents/Other Courses/Practical Machine Learning/Project/R/modelbuilding.R')
