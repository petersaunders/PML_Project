oliveTree = tree(Area ~ ., data=olive)
oliveTree
?tree
predict(oliveTree, newdata)
newdata = as.data.frame(t(colMeans(olive)))
predict(oliveTree, newdata)
table(olive$Area)
head(train)
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
install.packages(ElemStatLearn)
install.packages('ElemStatLearn'')
install.packages('ElemStatLearn')
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
head(trainSA)
set.seed(13234)
hdModel = train(chd ~ age+alcohol+obesity+tobacco+typea+ldl, method="glm", family="binomial")
hdModel = train(chd ~ age+alcohol+obesity+tobacco+typea+ldl, data=trainSA, method="glm", family="binomial")
hdModel
set.seed(13234)
hdModel = train(chd ~ age+alcohol+obesity+tobacco+typea+ldl, data=trainSA, method="glm", family="binomial")
trainPred = predict(hdModel, newdata=trainSA)
testPred  = predict(hdModel, newdata=testSA)
str(testPred)
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
trainMC = missClass(trainSA$chd, trainPred)
testMC  = missClass(testSA$chd, testPred)
trainMC
testMC
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
head(vowel.test)
head(vowel.train)
vowel.train$y = as.factor(vowel.train$y)
vowel.test$y = as.factor(vowel.test$y)
install.packages(randomForest)
install.packages('randomForest')
require(randomForest)
set.seed(33833)
vowelForest = randomForest(y ~., data=vowel.train)
str(vowelForest)
varImp(vowelForest, useModel = 0)
importance(vowelForest)
sort(importance(vowelForest))
important(vowelForest)[order(importance(vowelForest))]
importance(vowelForest)[order(importance(vowelForest))]
imps = importance(vowelForest)
imps
rownames(imps)
rownames(imps)[order(imps, decreasing = TRUE)]
require(caret)
setwd("D:/Documents/Other Courses/Practical Machine Learning/Project")
require(caret)
source("R/exploration.R")
set.seed(3485)
createDataPartition(trainSubDf$classe, p = 0.7, list = FALSE)
require(randomForest)
rf = randomForest(classe ~., data=trainPart)
#Separate into training and test (just once)
trainIdx = createDataPartition(trainSubDf$classe, p = 0.7, list = FALSE)
trainPart = trainSubDf[trainIdx, ]
testPart  = testSubDf[-trainIdx, ]
rf = randomForest(classe ~., data=trainPart)
#Separate into training and test (just once)
trainIdx = createDataPartition(trainSubDf$classe, p = 0.7, list = FALSE)
trainPart = trainSubDf[trainIdx, ]
testPart  = trainSubDf[-trainIdx, ]
rf = randomForest(classe ~., data=trainPart)
preds = predict(rf, newdata = testPart)
head(preds)
table("pred" = preds, "actual" = testPart$classe)
names(trainPArt)
names(trainPart)
ncol(trainPart)
set.seed(3485)
#Separate into training and test (just once)
trainIdx = createDataPartition(trainSubDf$classe, p = 0.7, list = FALSE)
trainPart = trainSubDf[trainIdx, ]
testPart  = trainSubDf[-trainIdx, ]
rf = randomForest(classe ~., data=trainPart[,6:58])
preds = predict(rf, newdata=testPart[, 6:58])
table("pred" = preds, "actual" = testPart$classe)
rfMod = rf
str(rfMod)
importance(rfMod)
cTab = table("pred" = preds, "actual" = testPart$classe)
cTab
diag(cTab)
sum(diag(cTab)) / nrow(testPart)
preds
#Separate into training and test (just once)
for (i in 1:10) {
trainIdx = createDataPartition(trainSubDf$classe, p = 0.7, list = FALSE)
trainPart = trainSubDf[trainIdx, ]
testPart  = trainSubDf[-trainIdx, ]
rfMod = randomForest(classe ~., data=trainPart[,6:58])
preds = predict(rfMod, newdata=testPart[, 6:58])
accuracy = length(which(preds == testPart$classe)) / nrow(testPart)
table("pred" = preds, "actual" = testPart$classe)
cat("Accuracy = ", accuracy, "\n", sep="")
}
for (i in 1:10) {
trainIdx = createDataPartition(trainSubDf$classe, p = 0.7, list = FALSE)
trainPart = trainSubDf[trainIdx, ]
testPart  = trainSubDf[-trainIdx, ]
rfMod = randomForest(classe ~., data=trainPart[,6:58])
preds = predict(rfMod, newdata=testPart[, 6:58])
accuracy = length(which(preds == testPart$classe)) / nrow(testPart)
cTab = table("pred" = preds, "actual" = testPart$classe)
print(cTab)
cat("Accuracy = ", accuracy, "\n", sep="")
}
kfolds <- createFolds(trainSubDf$classe, k = 50, list = TRUE, returnTrain = T)
str(kfolds)
?createFolds
set.seed(3485)
#Separate into training and test (just once)
K = 50
kfolds <- createFolds(trainSubDf$classe, k = K, list = TRUE, returnTrain = T)
for (i in 1:K) {
training = trainSubDf[kfolds[[i]], ]  #make training
testing  = trainSubDf[-kfolds[[i]], ] #make testing
#Make model
rfMod = randomForest(classe ~., data=training[,6:58])
#Evaluate on testing set
preds = predict(rfMod, newdata=testing[, 6:58])
accuracy = length(which(preds == testing$classe)) / nrow(testing)
cTab = table("pred" = preds, "actual" = testing$classe)
#print(cTab)
cat("Accuracy = ", accuracy, "\n", sep="")
}
?createFold
?createFolds
resubPred = predict(rfMod, newdata=trainSubDf)
head(resubPred)
all(resubPred == trainSubDf$classe)
length(which(resubPred != trainSubDf$classe))
which(resubPred != trainSubDf$classe)
importance(rfMod)
K = 500
kfolds <- createFolds(trainSubDf$classe, k = K, list = TRUE, returnTrain = T)
for (i in 1:K) {
training = trainSubDf[kfolds[[i]], ]  #make training
testing  = trainSubDf[-kfolds[[i]], ] #make testing
#Make model
rfMod = randomForest(classe ~., data=training[,6:58])
#Evaluate on testing set
preds = predict(rfMod, newdata=testing[, 6:58])
accuracy = length(which(preds == testing$classe)) / nrow(testing)
cTab = table("pred" = preds, "actual" = testing$classe)
#print(cTab)
cat("Accuracy = ", accuracy, "\n", sep="")
}
str(rfMod)
length(kfolds[[1]])
nrow(trainSubDf)
nrow(trainSubDf) - length(kfolds[[1]])
K = 20
kfolds <- createFolds(trainSubDf$classe, k = K, list = TRUE, returnTrain = TRUE)
nrow(trainSubDf) - length(kfolds[[1]])
#Separate into training and test (just once)
K = 20
kfolds <- createFolds(trainSubDf$classe, k = K, list = TRUE, returnTrain = TRUE)
for (i in 1:K) {
training = trainSubDf[kfolds[[i]], ]  #make training
testing  = trainSubDf[-kfolds[[i]], ] #make testing
#Make model
rfMod = randomForest(classe ~., data=training[,6:58])
#Evaluate on testing set
preds = predict(rfMod, newdata=testing[, 6:58])
accuracy = length(which(preds == testing$classe)) / nrow(testing)
cTab = table("pred" = preds, "actual" = testing$classe)
#print(cTab)
cat("Accuracy = ", accuracy, "\n", sep="")
}
#Separate into training and test (just once)
K = 10
kfolds <- createFolds(trainSubDf$classe, k = K, list = TRUE, returnTrain = TRUE)
for (i in 1:K) {
training = trainSubDf[kfolds[[i]], ]  #make training
testing  = trainSubDf[-kfolds[[i]], ] #make testing
#Make model
rfMod = randomForest(classe ~., data=training[,6:58])
#Evaluate on testing set
preds = predict(rfMod, newdata=testing[, 6:58])
accuracy = length(which(preds == testing$classe)) / nrow(testing)
cTab = table("pred" = preds, "actual" = testing$classe)
#print(cTab)
cat("Accuracy = ", accuracy, "\n", sep="")
}
K = 10
kfolds <- createFolds(trainSubDf$classe, k = K, list = TRUE, returnTrain = TRUE)
length(kfolds)
training = trainSubDf[kfolds[[i]], ]  #make training
testing  = trainSubDf[-kfolds[[i]], ] #make testing
nrow(training)
nrow(testing)
set.seed(3485)
#Separate into training and test (just once)
K = 10
kfolds <- createFolds(trainSubDf$classe, k = K, list = TRUE, returnTrain = TRUE)
for (i in 1:K) {
training = trainSubDf[kfolds[[i]], ]  #make training
testing  = trainSubDf[-kfolds[[i]], ] #make testing
#Make model
rfMod = randomForest(classe ~., data=training[,6:58])
#Evaluate on testing set
preds = predict(rfMod, newdata=testing[, 6:58])
accuracy = length(which(preds == testing$classe)) / nrow(testing)
cTab = table("pred" = preds, "actual" = testing$classe)
#print(cTab)
cat("Accuracy = ", accuracy, "\n", sep="")
}
set.seed(3485)
modelIdx = createDataPartition(trainSubDf$classe, p = 0.8, list = FALSE)
modelData       = trainSubDf[modelIdx,]
validationData  = trainSubDf[-modelIdx,]
nrow(modelData)
nrow(validationData)
View(modelData)
#Separate data into training and validation sets
set.seed(3485)
modelIdx = createDataPartition(trainSubDf$classe, p = 0.8, list = FALSE)
modelData       = trainSubDf[modelIdx,]
validationData  = trainSubDf[-modelIdx,]
#Now do k-fold cross-validation
K = 10
kfolds = createFolds(modelData$classe, k = K, list = TRUE, returnTrain = TRUE)
models = vector(length = K, mode = "list")
for (i in 1:K) {
training = modelData[kfolds[[i]], ]  #make training
testing  = modelData[-kfolds[[i]], ] #make testing
#Make model
rfMod = randomForest(classe ~., data=training[,6:58])
models[[i]] = rfMod
#Evaluate on testing set
preds = predict(rfMod, newdata=testing[, 6:58])
accuracy = length(which(preds == testing$classe)) / nrow(testing)
cTab = table("pred" = preds, "actual" = testing$classe)
#print(cTab)
cat("Accuracy = ", accuracy, "\n", sep="")
}
#Now combine the models
finalMod = combine(models)
validationPreds = predict(finalMod, newData = validationData[, 6:58])
validationAccuracy = length(which(validationPreds == validationData$classe)) / nrow(validationData)
cat("Final validation accuracy =", validationAccuracy)
#Separate data into training and validation sets
#and remove index, user and timestamp columns
set.seed(3485)
modelIdx = createDataPartition(trainSubDf$classe, p = 0.8, list = FALSE)
modelData       = trainSubDf[modelIdx, 6:58]
validationData  = trainSubDf[-modelIdx, 6:58]
#Separate data into training and validation sets
#and remove index, user and timestamp columns
set.seed(3485)
modelIdx = createDataPartition(trainSubDf$classe, p = 0.8, list = FALSE)
modelData       = trainSubDf[modelIdx, 6:58]
validationData  = trainSubDf[-modelIdx, 6:58]
#Now do k-fold cross-validation
K = 10
kfolds = createFolds(modelData$classe, k = K, list = TRUE, returnTrain = TRUE)
models = vector(length = K, mode = "list")
for (i in 1:K) {
training = modelData[kfolds[[i]], ]  #make training
testing  = modelData[-kfolds[[i]], ] #make testing
#Make model
rfMod = randomForest(classe ~., data=training)
models[[i]] = rfMod
#Evaluate on testing set
preds = predict(rfMod, newdata=testing)
accuracy = length(which(preds == testing$classe)) / nrow(testing)
cTab = table("pred" = preds, "actual" = testing$classe)
#print(cTab)
cat("Accuracy = ", accuracy, "\n", sep="")
}
rm()
require(caret)
source("R/exploration.R")
#Now do k-fold cross-validation
K = 10
kfolds = createFolds(modelData$classe, k = K, list = TRUE, returnTrain = TRUE)
models = vector(length = K, mode = "list")
for (i in 1:K) {
training = modelData[kfolds[[i]], ]  #make training
testing  = modelData[-kfolds[[i]], ] #make testing
#Make model
rfMod = randomForest(classe ~., data=training)
models[[i]] = rfMod
#Evaluate on testing set
preds = predict(rfMod, newdata=testing)
accuracy = length(which(preds == testing$classe)) / nrow(testing)
cTab = table("pred" = preds, "actual" = testing$classe)
#print(cTab)
cat("Accuracy = ", accuracy, "\n", sep="")
}
#Separate data into training and validation sets
#and remove index, user and timestamp columns
set.seed(3485)
modelIdx = createDataPartition(trainSubDf$classe, p = 0.8, list = FALSE)
modelData       = trainSubDf[modelIdx, 6:58]
validationData  = trainSubDf[-modelIdx, 6:58]
#Now do k-fold cross-validation
K = 10
kfolds = createFolds(modelData$classe, k = K, list = TRUE, returnTrain = TRUE)
models = vector(length = K, mode = "list")
for (i in 1:K) {
training = modelData[kfolds[[i]], ]  #make training
testing  = modelData[-kfolds[[i]], ] #make testing
#Make model
rfMod = randomForest(classe ~., data=training)
models[[i]] = rfMod
#Evaluate on testing set
preds = predict(rfMod, newdata=testing)
accuracy = length(which(preds == testing$classe)) / nrow(testing)
cTab = table("pred" = preds, "actual" = testing$classe)
#print(cTab)
cat("Accuracy = ", accuracy, "\n", sep="")
}
require(randomForest)
#Now do k-fold cross-validation
K = 10
kfolds = createFolds(modelData$classe, k = K, list = TRUE, returnTrain = TRUE)
models = vector(length = K, mode = "list")
for (i in 1:K) {
training = modelData[kfolds[[i]], ]  #make training
testing  = modelData[-kfolds[[i]], ] #make testing
#Make model
rfMod = randomForest(classe ~., data=training)
models[[i]] = rfMod
#Evaluate on testing set
preds = predict(rfMod, newdata=testing)
accuracy = length(which(preds == testing$classe)) / nrow(testing)
cTab = table("pred" = preds, "actual" = testing$classe)
#print(cTab)
cat("Accuracy = ", accuracy, "\n", sep="")
}
finalMod = combine(models)
are(moswla)
str(models)
class(models[[1]])
models2 = lapply(models, function(x)"$"(x, "randomForest"))
str(models2)
?combine
finalMod = combine(models)
?randomForest
#Now do k-fold cross-validation
K = 10
kfolds = createFolds(modelData$classe, k = K, list = TRUE, returnTrain = TRUE)
models = vector(length = K, mode = "list")
for (i in 1:K) {
training = modelData[kfolds[[i]], ]  #make training
testing  = modelData[-kfolds[[i]], ] #make testing
#Make model
rfMod = randomForest(classe ~., data=training, norm.votes=FALSE)
models[[i]] = rfMod$finalModel
#Evaluate on testing set
preds = predict(rfMod, newdata=testing)
accuracy = length(which(preds == testing$classe)) / nrow(testing)
cTab = table("pred" = preds, "actual" = testing$classe)
#print(cTab)
cat("Accuracy = ", accuracy, "\n", sep="")
}
#Now combine the models
finalMod = combine(models)
str(models)
str(rfMod)
importance(rfMod)
?combine
#Now do k-fold cross-validation
K = 10
kfolds = createFolds(modelData$classe, k = K, list = TRUE, returnTrain = TRUE)
models = vector(length = K, mode = "list")
for (i in 1:K) {
training = modelData[kfolds[[i]], ]  #make training
testing  = modelData[-kfolds[[i]], ] #make testing
#Make model
rfMod = randomForest(classe ~., data=training, ntree=100, norm.votes=FALSE)
models[[i]] = rfMod
#Evaluate on testing set
preds = predict(rfMod, newdata=testing)
accuracy = length(which(preds == testing$classe)) / nrow(testing)
cTab = table("pred" = preds, "actual" = testing$classe)
#print(cTab)
cat("Accuracy = ", accuracy, "\n", sep="")
}
finalMod = combine(models)
combine(models[[1]], models[[2]])
#Now do k-fold cross-validation
require(randomForest)
K = 15
kfolds = createFolds(modelData$classe, k = K, list = TRUE, returnTrain = TRUE)
models = vector(length = K, mode = "list")
for (i in 1:K) {
training = modelData[kfolds[[i]], ]  #make training
testing  = modelData[-kfolds[[i]], ] #make testing
#Make model and store it
rfMod = randomForest(classe ~., data=training, ntree=500, norm.votes=FALSE)
models[[i]] = rfMod
#Evaluate on testing set to print accuracy
preds = predict(rfMod, newdata=testing)
accuracy = length(which(preds == testing$classe)) / nrow(testing)
cat("Accuracy = ", accuracy, "\n", sep="")
}
#Now use all of the models to make predictions
validationPreds = lapply(models, predict, newData = validationData)
str(validationPreds)
validationPreds[[1]]
nrow(validationData)
length(validationPreds[[1]])
validationPreds = sapply(models, predict, newData = validationData,)
validationPreds = sapply(models, predict, newData = validationData)
str(validationPreds)
nrow(validationData)
nrow(trainSubDf)
nrow(modelData)
nrow(training)
p1 = predict(models[[1]], newdata=validationData)
length(p1)
validationPreds = lapply(models, predict, list('newData' = validationData))
#Now use all of the models to make predictions
validationPreds = lapply(models, predict, 'newData' = validationData)
length(validationPreds[[1]])
?lapply
p1 = predict(models[1], newdata=validationData)
length(p1)
str(p1)
head(p1)
validationPreds = lapply(models, predict, newData = validationData, simplify = "array")
str(validationPreds)
length(validationPreds[[1]])
p1 = predict(models[[1]], newdata=validationData)
length(p1)
validationPreds = lapply(models, function(x)predict(x, newData = validationData), simplify = "array")
str(validationPreds)
validationPreds = lapply(models, function(x)predict(x, newData = validationData))
str(validationPreds)
validationPreds = lapply(models, function(x){predict(x, newdata=validationData)})
str(validationPreds)
validationMatrix = matrix(unlist(validationPreds))
validationMatrix = matrix(unlist(validationPreds), nrow=K)
View(validationMatrix)
finalPredict = apply(validationMatrix, MARGIN = 2, mode)
?mode
?mean
vote <- function(x) {
ux <- unique(x)
ux[which.max(tabulate(match(x, ux)))]
}
finalPredict = apply(validationMatrix, MARGIN = 2, vote)
class(finalPredict)
validationAccuracy = length(which(finalPredict == validationData$classe)) / nrow(validationData)
validationAccuracy
head(finalPredict, 50)
head(validationData$classe)
head(validationData$classe, 50)
validationAccuracy = length(which(finalPredict == as.character(validationData$classe)) / nrow(validationData)
finalPredict = apply(validationMatrix, MARGIN = 2, vote)
validationAccuracy = length(which(finalPredict == as.character(validationData$classe))) / nrow(validationData)
cat("Final validation accuracy =", validationAccuracy, "\n")
validationMatrix[, 2350]
vote(validationMatrix[, 2350])
unique(finalPredict)
table(finalPredict)
length(which(finalPredict == as.character(validationData$classe)))
validationAccuracy = length(which(finalPredict != as.character(validationData$classe))) / nrow(validationData)
length(which(finalPredict != as.character(validationData$classe)))
vote <- function(x) {
ux = unique(x)
ux[which.max(tabulate(match(x, ux)))]
}
finalPredict = apply(validationMatrix, MARGIN = 2, vote)
view(data.frame(finalPredict, validationData$classe))
View(data.frame(finalPredict, validationData$classe))
length(finalPredict)
validationMatrix[, 200:205]
unique(validationData$classe)
nrow(validationData)
#Now use all of the models to make predictions
#TODO: Something is broken here... this definitely is not right!!!
validationPreds = lapply(models, function(x){predict(x, newdata=validationData)})
validationMatrix = matrix(unlist(validationPreds), ncol=K)
head(validationMatrix)
valiationMatrix[200:210]
validationMatrix[200:210,]
#Use consensus vote of each of the K models to form prediction
vote <- function(x) {
ux = unique(x)
ux[which.max(tabulate(match(x, ux)))]
}
finalPredict = apply(validationMatrix, MARGIN = 1, vote)
head(finalPredict)
finalPredict[200:220]
distinct(finalPredict)
unique(finalPredict)
validationAccuracy = length(which(finalPredict != as.character(validationData$classe))) / nrow(validationData)
cat("Final validation accuracy =", validationAccuracy, "\n")
validationPreds = lapply(models, function(x){predict(x, newdata=validationData)})
validationMatrix = unlist(validationPreds)
dim(validationMatrix)
class(validationMatrix)
length(validationMatrix)
